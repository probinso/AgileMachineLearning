{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from builtins import str\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML \n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "import os\n",
    "from decimal import Decimal\n",
    "from traceback import print_exc\n",
    "\n",
    "np = pd.np\n",
    "BASE_PATH = os.path.abspath(os.path.join('..', '..', '..'))\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'Data')\n",
    "MODELS_PATH = os.path.join(BASE_PATH, 'Models')\n",
    "sys.path.append(DATA_PATH)\n",
    "sys.path.append(MODELS_PATH)\n",
    "tld_iana = pd.read_csv(os.path.join(DATA_PATH, 'tlds-from-iana.csv'))\n",
    "tld_iana = collections.OrderedDict(sorted(zip((tld.strip().lstrip('.') for tld in tld_iana.domain),\n",
    "                                              [(sponsor.strip(), -1) for sponsor in tld_iana.sponsor]),\n",
    "                                          key=lambda x: len(x[0]),\n",
    "                                          reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# top 20 in Google searches per day\n",
    "# sorted by longest first so .com matches before .om (Oman)\n",
    "tld_popular = collections.OrderedDict(sorted([\n",
    "    ('com', ('Commercial', 4860000000)),\n",
    "    ('org', ('Noncommercial', 1950000000)),\n",
    "    ('edu', ('US accredited postsecondary institutions', 1550000000)),\n",
    "    ('gov', ('United States Government', 1060000000)),\n",
    "    ('uk',  ('United Kingdom', 473000000)),\n",
    "    ('net', ('Network services', 206000000)),\n",
    "    ('ca', ('Canada', 165000000)),\n",
    "    ('de', ('Germany', 145000000)),\n",
    "    ('jp', ('Japan', 139000000)),\n",
    "    ('fr', ('France', 96700000)),\n",
    "    ('au', ('Australia', 91000000)),\n",
    "    ('us', ('United States', 68300000)),\n",
    "    ('ru', ('Russian Federation', 67900000)),\n",
    "    ('ch', ('Switzerland', 62100000)),\n",
    "    ('it', ('Italy', 55200000)),\n",
    "    ('nl', ('Netherlands', 45700000)),\n",
    "    ('se', ('Sweden', 39000000)),\n",
    "    ('no', ('Norway', 32300000)),\n",
    "    ('es', ('Spain', 31000000)),\n",
    "    ('mil', ('US Military', 28400000)),\n",
    "    ], key=lambda x: len(x[0]), reverse=True))\n",
    "uri_schemes_iana = sorted(pd.read_csv(os.path.join(DATA_PATH, 'uri-schemes.xhtml.csv'),\n",
    "                                      index_col=0).index.values,\n",
    "                          key=lambda x: len(str(x)), reverse=True)\n",
    "uri_schemes_popular = ['chrome-extension', 'example', 'content', 'bitcoin',\n",
    "                       'telnet', 'mailto',\n",
    "                       'https', 'gtalk',\n",
    "                       'http', 'smtp', 'feed',\n",
    "                       'udp', 'ftp', 'ssh', 'git', 'apt', 'svn', 'cvs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16039</td>\n",
       "      <td>16039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>750</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41983</td>\n",
       "      <td>34885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tf     df\n",
       "count  16039  16039\n",
       "mean      85     78\n",
       "std      750    643\n",
       "...      ...    ...\n",
       "50%       12     12\n",
       "75%       31     31\n",
       "max    41983  34885\n",
       "\n",
       "[8 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf = pd.read_csv(os.path.join(DATA_PATH, 'tweet_vocab.csv.gz'), index_col=0, compression='gzip',\n",
    "                   quotechar='\"', quoting=pd.io.common.csv.QUOTE_NONNUMERIC, low_memory=False)\n",
    "tfdf.describe().round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to allocate a 16k word by 100k document DataFrame of 64-bit integers, you'll get a memory error on a 16 GB laptop.  \n",
    "Later we'll learn about \"constant RAM\" tools that can handle an unlimitted stream of documents with a large (1M word) vocabulary. But first let's be frugal and see what we can do with robust, mature tools like Pandas.  \n",
    "Rather than cutting back on those 100k tweets, lets cut back on the words. What are all those 16k words and how often are they all used (maybe we can ignore infrequent words).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8312"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = 8 * (100 * 1000 * len(tfdf)) / 1.e9\n",
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1417</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>355</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>610</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzisqvsjbf</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzp</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzrkwxgbqv</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16039 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tf    df\n",
       "0           1417  1395\n",
       "0.0          355   354\n",
       "0.1          610   604\n",
       "...          ...   ...\n",
       "zzisqvsjbf     6     6\n",
       "zzp           14     9\n",
       "zzrkwxgbqv    21    21\n",
       "\n",
       "[16039 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately the odd words are at the top and bottom of an alphabetical index!  \n",
    "And it does look like the less useful tokens aren't used many times or in many documents.  \n",
    "What do you notice that might help distinguish \"natural\" words (zoom, zoos, zope, zynga) from URLs and machine-code (000, zzp, zsl107)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1417</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>355</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>610</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoos</th>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zy0nsstslv</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzrkwxgbqv</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5391 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tf    df\n",
       "0           1417  1395\n",
       "0.0          355   354\n",
       "0.1          610   604\n",
       "...          ...   ...\n",
       "zoos          63    63\n",
       "zy0nsstslv    27    27\n",
       "zzrkwxgbqv    21    21\n",
       "\n",
       "[5391 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf = tfdf[tfdf.df > 9]\n",
    "tfdf = tfdf[(tfdf.df > 9) & (((tfdf.df - tfdf.tf) / tfdf.tf) < 0.15)]\n",
    "tfdf = tfdf[(tfdf.df > 20) & (((tfdf.df - tfdf.tf) / tfdf.tf) < 0.15)]\n",
    "tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-2a632d3ab850>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-2a632d3ab850>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Numpy arrays (guts of Pandas DataFrame) require 8 bytes for each double-precision value (int64)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Numpy arrays (guts of Pandas DataFrame) require 8 bytes for each double-precision value (int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = 8 * (100 * 1000 * len(tfdf)) / 1.e9\n",
    "GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory requirements (4 GB) are doable  \n",
    "But we've lost important words: **\"zoom\"**  \n",
    "And there's still a bit of garbage: **\"zh3gs0wbno\"**  \n",
    "These look like keys, slugs, hashes or URLs  \n",
    "Even though the tweets.json format includes a column for URLs  \n",
    "The URLs are left within the raw text as well  \n",
    "Let's use a formal but simple grammar engine:\n",
    "\n",
    "## Extended regular expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ms-secondary-screen-controller\n",
       "1      ms-settings-connectabledevices\n",
       "2       ms-settings-displays-topology\n",
       "3        ms-settings-emailandaccounts\n",
       "4         ms-settings-nfctransactions\n",
       "5          ms-settings-screenrotation\n",
       "6           ms-secondary-screen-setup\n",
       "                    ...              \n",
       "237                                aw\n",
       "238                                gg\n",
       "239                                go\n",
       "240                                im\n",
       "241                                ni\n",
       "242                                tv\n",
       "243                                ws\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constant.uri_schemes_popular = ['chrome', 'https', 'http', ]\n",
    "url_scheme_popular = r'(\\b(' + '|'.join(uri_schemes_popular) + r')[:][/]{2})'\n",
    "fqdn_popular = r'(\\b[a-zA-Z0-9-.]+\\b([.]' + r'|'.join(tld_popular) + r'\\b)\\b)'\n",
    "url_path = r'(\\b[\\w/?=+#-_&%~\\'\"\\\\.,]*\\b)'\n",
    "\n",
    "pd.set_option('display.max_rows', 14)\n",
    "pd.Series(uri_schemes_iana)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://totalgood.com/a/b?c=42',\n",
       "  'http',\n",
       "  'totalgood.com',\n",
       "  '.com',\n",
       "  '/a/b?c=42'),\n",
       " ('svn://us.gov', 'svn', 'us.gov', 'gov', '')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_popular = r'(\\b' + r'(http|https|svn|git|apt)[:]//' + fqdn_popular + url_path + r'\\b)'\n",
    "tweet = \"Play the [postiive sum game](http://totalgood.com/a/b?c=42) of life instead of svn://us.gov.\"\n",
    "import re\n",
    "re.findall(url_popular, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# email = re.compile(r'^([\\w-]+(?:\\.[\\w-]+)*)@((?:[\\w-]+\\.)*\\w[\\w-]{0,66})\\.([a-z]{2,6}(?:\\.[a-z]{2})?)')\n",
    "fqdn = r'(\\b[a-zA-Z0-9-.]+([.]' + r'|'.join(tld_iana) + r')\\b)'\n",
    "fqdn_popular = r'(\\b[a-zA-Z0-9-.]+\\b([.]' + r'|'.join(tld_popular) + r'\\b)\\b)'\n",
    "username = r'(\\b[a-zA-Z0-9-.!#$%&*+-/=?^_`{|}~]+\\b)'\n",
    "\n",
    "email = re.compile(r'(\\b' + username + r'\\b@\\b' + fqdn + r'\\b)')\n",
    "email_popular = re.compile(r'(\\b' + username + r'\\b@\\b' + fqdn_popular + r'\\b)')\n",
    "\n",
    "# TODO: unmatched surrounding symbols are accepted/consumed, likewise for multiple dots/ats\n",
    "at = r'(([-@=\"_(\\[{\\|\\s]+(at|At|AT)[-@=\"_)\\]\\}\\|\\s]+)|[@])'\n",
    "dot = r'(([-.=\"_(\\[{\\|\\s]+(dot|dt|Dot|DOT)[-.=\"_)\\]\\}\\|\\s]+)|[.])'\n",
    "fqdn_obfuscated = r'(\\b(([a-zA-Z0-9-]+' + dot + r'){1,7})(' + r'|'.join(tld_iana) + r')\\b)'\n",
    "fqdn_popular_obfuscated = r'(\\b(([a-zA-Z0-9-]+' + dot + r'){1,7})(' + r'|'.join(tld_popular) + r')\\b)'\n",
    "username_obfuscated = r'(([a-zA-Z0-9!#$%&*+/?^`~]+' + dot + r'?){1,7})'\n",
    "email_obfuscated = re.compile(r'(\\b' + username_obfuscated + at + fqdn_obfuscated + r'\\b)')\n",
    "email_popular_obfuscated = re.compile(r'(\\b' + username_obfuscated + at + fqdn_popular_obfuscated + r'\\b)')\n",
    "\n",
    "url_path = r'(\\b[^\\s]+)'\n",
    "url_scheme = r'(\\b(' + '|'.join(uri_schemes_iana) + r')[:][/]{2})'\n",
    "url_scheme_popular = r'(\\b(' + '|'.join(uri_schemes_popular) + r')[:][/]{2})'\n",
    "url = r'(\\b' + url_scheme + fqdn + url_path + r'?\\b)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
